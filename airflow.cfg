[core]
dags_folder = /opt/airflow/dags
hostname_callable = airflow.utils.net.get_host_ip_address
#default_timezone = utc
executor = CeleryExecutor
sql_alchemy_conn = postgresql+psycopg2://airflow_user:airflow_user@rpi.home.net:5432/airflow_db
#sql_engine_encoding = utf-8
#sql_alchemy_pool_enabled = True
#sql_alchemy_pool_size = 5
#sql_alchemy_max_overflow = 10
#sql_alchemy_pool_recycle = 1800
#sql_alchemy_pool_pre_ping = True
#sql_alchemy_schema =
#parallelism = 32
#dag_concurrency = 16
#dags_are_paused_at_creation = True
#max_active_runs_per_dag = 16
load_examples = False
#load_default_connections = False
#plugins_folder = /opt/airflow/plugins
#execute_tasks_new_python_interpreter = False
#fernet_key = q2_ozoBrpn7JmoWJRMJAq-R06w5ORx2obgZPxLpSu8E=
#donot_pickle = True
#dagbag_import_timeout = 30.0
#dagbag_import_error_tracebacks = True
#dagbag_import_error_traceback_depth = 2
#dag_file_processor_timeout = 50
#task_runner = StandardTaskRunner
#default_impersonation =
#security =
#unit_test_mode = False
#enable_xcom_pickling = True
#killed_task_cleanup_time = 60
#dag_run_conf_overrides_params = True
#dag_discovery_safe_mode = True
#default_task_retries = 0
#min_serialized_dag_update_interval = 30
#min_serialized_dag_fetch_interval = 10
#max_num_rendered_ti_fields_per_task = 30
#check_slas = True
#xcom_backend = airflow.models.xcom.BaseXCom
#lazy_load_plugins = True
#lazy_discover_providers = True
#max_db_retries = 3

[logging]
#base_log_folder = /opt/airflow/logs
#remote_logging = False
#remote_log_conn_id =
#google_key_path =
#remote_base_log_folder =
#encrypt_s3_logs = False
#logging_level = INFO
#fab_logging_level = WARN
#logging_config_class =
#colored_console_log = True
#colored_log_format = [%%(blue)s%%(asctime)s%%(reset)s] {%%(blue)s%%(filename)s:%%(reset)s%%(lineno)d} %%(log_color)s%%(levelname)s%%(reset)s - %%(log_color)s%%(message)s%%(reset)s
#colored_formatter_class = airflow.utils.log.colored_log.CustomTTYColoredFormatter
#log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
#simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s
#task_log_prefix_template =
#log_filename_template = {{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log
#log_processor_filename_template = {{ filename }}.log
#dag_processor_manager_log_location = /opt/airflow/logs/dag_processor_manager/dag_processor_manager.log
#task_log_reader = task
#extra_loggers =

[metrics]
#statsd_on = False
#statsd_host = localhost
#statsd_port = 8125
#statsd_prefix = airflow
#statsd_allow_list =
#stat_name_handler =
#statsd_datadog_enabled = False
#statsd_datadog_tags =

[secrets]
#backend =
#backend_kwargs =

[cli]
#api_client = airflow.api.client.local_client
#endpoint_url = http://localhost:8080

[debug]
#fail_fast = False

[api]
#enable_experimental_api = False
#auth_backend = airflow.api.auth.backend.deny_all
#maximum_page_limit = 100
#fallback_page_limit = 100
#google_oauth2_audience =
#google_key_path =

[lineage]
#backend =

[atlas]
#sasl_enabled = False
#host =
#port = 21000
#username =
#password =

[operators]
#default_owner = airflow
#default_cpus = 1
#default_ram = 512
#default_disk = 512
#default_gpus = 0
#allow_illegal_arguments = False

[hive]
#default_hive_mapred_queue =

[webserver]
base_url = http://rpi.home.net:8080
#default_ui_timezone = UTC
web_server_host = rpi.home.net
web_server_port = 8080
#web_server_ssl_cert =
#web_server_ssl_key =
#web_server_master_timeout = 120
#web_server_worker_timeout = 120
#worker_refresh_batch_size = 1
#worker_refresh_interval = 30
#reload_on_plugin_change = False
#secret_key = C75K8C2p7qRWxsjhdbMGEA==
#workers = 4
#worker_class = sync
#access_logfile = -
#error_logfile = -
#access_logformat =
#expose_config = False
#expose_hostname = True
#expose_stacktrace = True
#dag_default_view = graph
#dag_orientation = LR
#demo_mode = False
#log_fetch_timeout_sec = 5
#log_fetch_delay_sec = 2
#log_auto_tailing_offset = 30
#log_animation_speed = 1000
#hide_paused_dags_by_default = False
#page_size = 100
#navbar_color = #fff
#default_dag_run_display_number = 25
#enable_proxy_fix = False
#proxy_fix_x_for = 1
#proxy_fix_x_proto = 1
#proxy_fix_x_host = 1
#proxy_fix_x_port = 1
#proxy_fix_x_prefix = 1
#cookie_secure = False
#cookie_samesite = Lax
#default_wrap = False
#x_frame_enabled = True
#show_recent_stats_for_completed_runs = True
#update_fab_perms = True
#session_lifetime_minutes = 43200

[email]
email_backend = airflow.providers.sendgrid.utils.emailer.send_email
#default_email_on_retry = True
#default_email_on_failure = True
#subject_template = ~/airflow/templates/subject_template
#html_content_template = ~/airflow/templates/content_template

[smtp]
#smtp_host = localhost
#smtp_starttls = True
#smtp_ssl = False
#smtp_port = 25
#smtp_mail_from = airflow@example.com
#smtp_timeout = 30
#smtp_retry_limit = 5

[sentry]
#sentry_on = false
#sentry_dsn =

[celery_kubernetes_executor]
#kubernetes_queue = kubernetes

[celery]
#celery_app_name = airflow.executors.celery_executor
#worker_concurrency = 4
#worker_log_server_port = 8793
#worker_umask = 0o077
broker_url = redis://rpi.home.net:6379/0
result_backend = db+postgresql://airflow_user:airflow_user@rpi.home.net:5432/airflow_celery_results
#flower_host = 0.0.0.0
#flower_url_prefix =
#flower_port = 5555
#flower_basic_auth =
#default_queue = default
#sync_parallelism = 0
#celery_config_options = airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG
#ssl_active = False
#ssl_key =
#ssl_cert =
#ssl_cacert =
#pool = prefork
#operation_timeout = 1.0
#task_track_started = True
#task_adoption_timeout = 600
#task_publish_max_retries = 3
#worker_precheck = True

[celery_broker_transport_options]

[dask]
#cluster_address = 127.0.0.1:8786
#tls_ca =
#tls_cert =
#tls_key =

[scheduler]
job_heartbeat_sec = 5
#clean_tis_without_dagrun_interval = 15.0
scheduler_heartbeat_sec = 180
#num_runs = -1
#processor_poll_interval = 1
#min_file_process_interval = 0
dag_dir_list_interval = 30
print_stats_interval = 30
#pool_metrics_interval = 5.0
scheduler_health_check_threshold = 30
#orphaned_tasks_check_interval = 300.0
#child_process_log_directory = /opt/airflow/logs/scheduler
#scheduler_zombie_task_threshold = 300
#catchup_by_default = False
#max_tis_per_query = 512
#use_row_level_locking = True
#parsing_processes = 2
#use_job_schedule = True
#allow_trigger_in_future = False

[kerberos]
#ccache = /tmp/airflow_krb5_ccache
#principal = airflow
#reinit_frequency = 3600
#kinit_path = kinit
#keytab = airflow.keytab

[github_enterprise]
#api_rev = v3

[admin]
#hide_sensitive_variable_fields = True
#sensitive_variable_fields =

[elasticsearch]
#host =
#log_id_template = {dag_id}-{task_id}-{execution_date}-{try_number}
#end_of_log_mark = end_of_log
#frontend =
#write_stdout = False
#json_format = False
#json_fields = asctime, filename, lineno, levelname, message

[elasticsearch_configs]
#use_ssl = False
#verify_certs = True

[kubernetes]
#pod_template_file =
#worker_container_repository =
#worker_container_tag =
#namespace = default
#delete_worker_pods = True
#delete_worker_pods_on_failure = False
#worker_pods_creation_batch_size = 1
#multi_namespace_mode = False
#in_cluster = True
#kube_client_request_args =
#delete_option_kwargs =
#enable_tcp_keepalive = False
#tcp_keep_idle = 120
#tcp_keep_intvl = 30
#tcp_keep_cnt = 6

[smart_sensor]
#use_smart_sensor = False
#shard_code_upper_limit = 10000
#shards = 5
#sensors_enabled = NamedHivePartitionSensor

